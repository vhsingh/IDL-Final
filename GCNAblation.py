# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DcU-2SYDCsyfQrkj0iWy6xXIb3BR66d7
"""

# !pip install torch_geometric

import pandas as pd
import torch
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from torch_geometric.data import Data
import os
import pickle
import glob
import shutil
from tqdm import tqdm
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GCNConv, global_mean_pool
from torch_geometric.data import DataLoader
from torch.optim import Adam
import random
from scipy.spatial.distance import directed_hausdorff
from scipy.spatial import procrustes
import scipy.io as io

# def clear_folder(folder_path):
#     # Check if the folder exists
#     if os.path.exists(folder_path):
#         # Remove all files and folders within the directory
#         for filename in os.listdir(folder_path):
#             file_path = os.path.join(folder_path, filename)
#             try:
#                 if os.path.isfile(file_path) or os.path.islink(file_path):
#                     os.unlink(file_path)
#                 elif os.path.isdir(file_path):
#                     shutil.rmtree(file_path)
#             except Exception as e:
#                 print('Failed to delete %s. Reason: %s' % (file_path, e))
#     else:
#         # If the folder doesn't exist, create it
#         os.makedirs(folder_path)

# def prepare_item_for_excel(item_element):
#     # Flatten the single element (item) so each coordinate set (x, y, z) is a row in the output DataFrame
#     flattened = item_element.reshape(-1, 3)  # Reshape: Flatten and then organize into rows of 3 values each
#     return pd.DataFrame(flattened)

# def prepare_1d_item_for_excel(item_element):
#     # Prepare the 1D element data as a DataFrame
#     return pd.DataFrame(item_element.reshape(-1,1), columns=['Value'])

# def process_data(file_path, base_folder, num_1d_data):

#     clear_folder(base_folder)

#     # Ensure the base folder exists
#     if not os.path.exists(base_folder):
#         os.makedirs(base_folder)

#     # Load the pickle file
#     with open(file_path, 'rb') as f:
#         data = pickle.load(f)

#     # Base path for saving Excel files
#     base_excel_file_path = os.path.join(base_folder, 'Element_{}.xlsx')

#     # Loop over the first 10 elements
#     for element_index in range(18600):
#         # Create an Excel writer object for this element
#         excel_file_path = base_excel_file_path.format(element_index)
#         with pd.ExcelWriter(excel_file_path, engine='openpyxl') as writer:
#             # Process each of the first 3 3D data items for this element
#             for item_index, item in enumerate(data[:3]):
#                 item_element = item[element_index]  # This is now a single (17, 12, 3) element
#                 df = prepare_item_for_excel(item_element)
#                 df.to_excel(writer, sheet_name=f'Item_{item_index}', index=False)

#             # Process the 1D data items for this element based on the file type
#             for item_index in range(3, 3 + num_1d_data):
#                 item_element = data[item_index][element_index]  # This is a single 1D element
#                 df = prepare_1d_item_for_excel(item_element)
#                 df.to_excel(writer, sheet_name=f'Property_{item_index-3}', index=False)

#         print(f"Excel file for Element {element_index} prepared and saved at {excel_file_path}")

# # File paths
# input_file_path = '/home/vedant/input_data.pkl'
# output_file_path = '/home/vedant/output_data.pkl'

# # Folder paths
# input_folder = '/home/vedant/Input_Excel'
# output_folder = '/home/vedant/Output_Excel'

# # Process each file with respective number of 1D data items
# process_data(input_file_path, input_folder, 4)  # Input data has 4 1D data items
# process_data(output_file_path, output_folder, 1)  # Output data has 1 1D data item

# # Define the paths to the input and output Excel folders
# input_folder_path = "/home/vedant/Input_Excel"
# output_folder_path = "/home/vedant/Output_Excel"

# # Get lists of Excel files in both folders
# input_files = glob.glob(os.path.join(input_folder_path, '*.xlsx'))
# output_files = glob.glob(os.path.join(output_folder_path, '*.xlsx'))

# # Sort files to ensure matching order
# input_files.sort()
# output_files.sort()

# # Initialize a dictionary to hold the graph data objects
# graph_data_objects = {}
# # Dictionary to store node and edge data for output files for visualization
# visualization_data_output = {}

# # Define the group size for adjacency pairs
# group_size = 17

# # Processing input files to create graph data objects
# for input_file, output_file in zip(input_files, output_files):
#     for sheet_idx in range(3):  # Handle only the first three sheets
#         input_df = pd.read_excel(input_file, sheet_name=sheet_idx)
#         output_df = pd.read_excel(output_file, sheet_name=sheet_idx)
#         # Read additional properties and extract the scalar value
#         pressure_value = pd.read_excel(input_file, sheet_name='Property_0')['Value'].iloc[0]
#         thickness_value = pd.read_excel(input_file, sheet_name='Property_1')['Value'].iloc[0]
#         coaptation_value = pd.read_excel(output_file, sheet_name='Property_0')['Value'].iloc[0]

#         # When you extract the scalar value from the DataFrame, ensure it's a tensor
#         pressure_tensor = torch.tensor([pressure_value], dtype=torch.float)
#         thickness_tensor = torch.tensor([thickness_value], dtype=torch.float)
#         coaptation_tensor = torch.tensor([coaptation_value], dtype=torch.float)

#         # Extract the first three columns for input tensors
#         x = torch.tensor(input_df.iloc[:, [0, 1, 2]].values, dtype=torch.float)
#         y = torch.tensor(output_df.iloc[:, [0, 1, 2]].values, dtype=torch.float)

#         src_indices = []
#         dst_indices = []
#         for i in range(len(input_df)):
#             if (i + 1) % group_size != 0 and (i + 1) < len(input_df):
#                 src_indices.append(i)
#                 dst_indices.append(i + 1)
#             if i < len(input_df) - group_size:
#                 src_indices.append(i)
#                 dst_indices.append(i + group_size)

#         edge_index = torch.tensor([src_indices, dst_indices], dtype=torch.long)

#         graph_data_object = Data(x=x, edge_index=edge_index, y=y,
#                          pressure=pressure_tensor,
#                          thickness=thickness_tensor,
#                          coaptation=coaptation_tensor)
#         item_name = f"{os.path.basename(input_file)}_Sheet_{sheet_idx+1}"
#         graph_data_objects[item_name] = graph_data_object

# # Define the directory to save the graph data objects
# save_dir = "graph_data_objects"
# os.makedirs(save_dir, exist_ok=True)  # Create the directory if it doesn't exist
# for key, graph_data in graph_data_objects.items():
#     file_path = os.path.join(save_dir, f"{key}.pt")  # Define the file path with .pt extension
#     torch.save(graph_data, file_path)  # Use torch.save for serialization

# # Print the created graph data objects to verify
# for name, graph_data in graph_data_objects.items():
#     print(f"Graph Data Object for {name}:")
#     print(graph_data)
#     print()  # Newline for readability

class EnhancedGraphEncoder(nn.Module):
    def __init__(self):
        super(EnhancedGraphEncoder, self).__init__()
        self.conv1 = GCNConv(3, 8)
        self.bn1 = nn.BatchNorm1d(8)  # Apply Batch Normalization
        self.dropout1 = nn.Dropout(0.25)  # Apply Dropout

        self.conv2 = GCNConv(8, 16)
        self.bn2 = nn.BatchNorm1d(16)
        self.dropout2 = nn.Dropout(0.25)

        self.conv3 = GCNConv(16, 32)
        self.bn3 = nn.BatchNorm1d(32)
        self.dropout3 = nn.Dropout(0.25)

    def forward(self, x, edge_index):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.bn1(x)
        x = self.dropout1(x)

        x = self.conv2(x, edge_index)
        x = F.relu(x)
        x = self.bn2(x)
        x = self.dropout2(x)

        x = self.conv3(x, edge_index)
        x = F.relu(x)
        x = self.bn3(x)
        x = self.dropout2(x)

        return x

class SimpleGraphDecoder(nn.Module):
    def __init__(self):
        super(SimpleGraphDecoder, self).__init__()
        self.fc1 = nn.Linear(32 + 2, 32)  # Adjusting to new feature size from conv2
        self.fc2 = nn.Linear(32, 32)
        self.fc3 = nn.Linear(32,16)
        self.fc4 = nn.Linear(16,16)
        self.fc5 = nn.Linear(16,8)
        self.fc6 = nn.Linear(8,8)
        self.fc7 = nn.Linear(8,1)

    def forward(self, x):
        x = self.fc1(x)
        x = F.relu(x)
        x = self.fc2(x)
        x = F.relu(x)
        x = self.fc3(x)
        x = F.relu(x)
        x = self.fc4(x)
        x = F.relu(x)
        x = self.fc5(x)
        x = F.relu(x)
        x = self.fc6(x)
        x = F.relu(x)
        x = self.fc7(x)
        return x


# class SimpleNodeDecoder(nn.Module):
#     def __init__(self):
#         super(SimpleNodeDecoder, self).__init__()
#         self.fc1 = nn.Linear(32, 64)  # Adjusting to new feature size from conv2
#         self.dropout1 = nn.Dropout(0.2)
#         self.fc2 = nn.Linear(64, 64)
#         self.bn2 = nn.BatchNorm1d(64)
#         self.fc3 = nn.Linear(64,64)
#         self.dropout3 = nn.Dropout(0.2)
#         self.fc4 = nn.Linear(64,32)
#         self.fc5 = nn.Linear(32,16)
#         self.fc6 = nn.Linear(16,8)
#         self.fc7 = nn.Linear(8,3)

#     def forward(self, x):
#         x = F.relu(self.fc1(x))
#         x = self.dropout1(x)
#         x = F.relu(self.fc2(x))
#         x = self.bn2(x)
#         x = F.relu(self.fc3(x))
#         x = self.dropout3(x)
#         x = F.relu(self.fc4(x))
#         x = F.relu(self.fc5(x))
#         x = F.relu(self.fc6(x))
#         x = self.fc7(x)
#         return x
class SimpleNodeDecoder(nn.Module):
    def __init__(self):
        super(SimpleNodeDecoder, self).__init__()
        self.fc1 = nn.Linear(32, 64)
        self.dropout1 = nn.Dropout(0.2)
        self.fc2 = nn.Linear(64, 64)
        self.bn2 = nn.BatchNorm1d(64)
        self.fc3 = nn.Linear(64, 64)
        self.dropout3 = nn.Dropout(0.2)
        self.fc4 = nn.Linear(64, 32)
        self.fc5 = nn.Linear(32, 16)
        self.fc6 = nn.Linear(16, 8)
        self.fc7 = nn.Linear(8, 3)

    def forward(self, x):
        identity = x
        x = F.relu(self.fc1(x))
        x = self.dropout1(x)
        x = F.relu(self.fc2(x))
        x = self.bn2(x)

        # Adding a residual connection
        if x.size(1) == identity.size(1):
            x = x + identity  # Element-wise addition
        
        x = F.relu(self.fc3(x))
        x = self.dropout3(x)
        x = F.relu(self.fc4(x))
        x = F.relu(self.fc5(x))
        x = F.relu(self.fc6(x))
        x = self.fc7(x)
        return x
    
class SimpleNodeDecoder(nn.Module):
    def __init__(self):
        super(SimpleNodeDecoder, self).__init__()
        self.fc1 = nn.Linear(32, 64)
        self.dropout1 = nn.Dropout(0.2)
        self.fc2 = nn.Linear(64, 64)
        self.bn2 = nn.BatchNorm1d(64)
        self.fc3 = nn.Linear(64, 64)
        self.dropout3 = nn.Dropout(0.2)
        self.fc4 = nn.Linear(64, 32)
        self.fc5 = nn.Linear(32, 32)
        self.fc6 = nn.Linear(32, 32)
        self.fc7 = nn.Linear(32, 16)
        self.fc8 = nn.Linear(16,16)
        self.fc9 = nn.Linear(16,8)
        self.fc10 = nn.Linear(8,8)
        self.fc11 = nn.Linear(8,3)

    def forward(self, x):
        # First block with potential residual connection
        identity = x
        x = F.relu(self.fc1(x))
        x = self.dropout1(x)
        x = F.relu(self.fc2(x))
        x = self.bn2(x)
        if x.size(1) == identity.size(1):
            x = x+ identity  # Element-wise addition

        # Process through more layers
        x = F.relu(self.fc3(x))
        x = self.dropout3(x)
        x = F.relu(self.fc4(x))

        # Second block with potential residual connection
        identity = x
        x = F.relu(self.fc5(x))
        if x.size(1) == identity.size(1):
            x = x + identity  # Element-wise addition

        x = F.relu(self.fc6(x))

        # Third block with potential residual connection
        identity = x
        x = F.relu(self.fc7(x))
        x = self.fc8(x)
        if x.size(1) == identity.size(1):
            x = x + identity  # Element-wise addition

        x = F.relu(self.fc9(x))
        x = F.relu(self.fc10(x))
        x = self.fc11(x)

        return x

class SimpleGNNModel(nn.Module):
    def __init__(self):
        super(SimpleGNNModel, self).__init__()
        self.encoder = EnhancedGraphEncoder()
        # Decoder for node level deformation predictions
        # self.node_decoder = nn.Linear(32, 3)  # Assuming each node has 3D deformation info
        self.node_decoder = SimpleNodeDecoder()
        # Decoder for graph level coaptation prediction
        # self.graph_decoder = nn.Sequential(
        #     nn.Linear(32 + 2, 64),  # Incorporate pressure and thickness features
        #     nn.ReLU(),
        #     nn.Dropout(0.25),
        #     nn.Linear(64, 1)
        # )
        self.graph_decoder = SimpleGraphDecoder()
    def forward(self, data, return_intermediate = False):
        x, edge_index, batch = data.x.to(device), data.edge_index.to(device), data.batch.to(device)
        pressure = data.pressure.to(device).unsqueeze(1)
        thickness = data.thickness.to(device).unsqueeze(1)

        # Encode node features
        encoded_features = self.encoder(x, edge_index)

        if return_intermediate:
            return encoded_features


        # Node-level predictions (deformation)
        node_predictions = self.node_decoder(encoded_features)

        # Graph-level features and prediction (coaptation)
        pooled_features = global_mean_pool(encoded_features, batch)
        graph_features = torch.cat([pooled_features, pressure, thickness], dim=1)
        graph_prediction = self.graph_decoder(graph_features)

        return node_predictions, graph_prediction

# Device setup
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = SimpleGNNModel().to(device)

# Parameter count
def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"Simplified model has {count_parameters(model):,} trainable parameters.")

# Seed for reproducibility
# random.seed(42)
# Define the directory where the graph data objects are saved
save_dir = "graph_data_objects"

# Initialize an empty dictionary to store the graph data objects
graph_data_objects = {}

# List all files in the directory
for filename in os.listdir(save_dir):
    if filename.endswith(".pt"):  # Check for .pt files
        file_path = os.path.join(save_dir, filename)
        # Load the graph data object
        graph_data = torch.load(file_path)
        # Extract the key from the filename (remove the .pt extension)
        key = filename[:-3]
        # Add the graph data object to the dictionary
        graph_data_objects[key] = graph_data
# Convert the dictionary of graph data objects into a list for easy handling
data_list = list(graph_data_objects.values())

# Randomize the data list
# random.shuffle(data_list)

# Calculate split sizes
total_length = len(data_list)
print(total_length)
train_size = int(0.6 * total_length)
print(train_size)
val_size = int(0.2 * total_length)
print(val_size)

# Split the data
train_data = data_list[:train_size]
print(len(train_data))
val_data = data_list[train_size:train_size + val_size]
print(len(val_data))
test_data = data_list[train_size + val_size:]

train_loader = DataLoader(train_data, batch_size=3, shuffle=True)
val_loader = DataLoader(val_data, batch_size=3, shuffle=False)
test_loader = DataLoader(test_data, batch_size=3, shuffle=False)

# Move model to GPU if available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

# Setup the optimizer
optimizer = Adam(model.parameters(), lr=0.001)

# Loss function
criterion_node = nn.MSELoss()
criterion_graph = nn.MSELoss()
# def disp_loss(y_true, y_pred, clip_delta=1.0):
#     # Compute squared error with scaling
#     se1 = torch.sum(((1.0 + y_true) / torch.max(1.0 + y_true)) * torch.square(y_pred - y_true), dim=-1)
#     # Compute mean squared error
#     se2 = torch.mean(torch.square(y_pred - y_true), dim=-1)
#     # Combine the errors and sum to make sure a scalar is returned
#     return (se1 + se2).sum()
print(device)

from scipy.spatial.distance import directed_hausdorff

def hausdorff_distance(u, v):
    return max(directed_hausdorff(u, v)[0], directed_hausdorff(v, u)[0])

from scipy.spatial import procrustes

def procrustes_distance(matrix1, matrix2):
    mtx1, mtx2, disparity = procrustes(matrix1, matrix2)
    # The disparity returned by scipy.spatial.procrustes is the sum of the squared differences
    # between the transformed matrix1 and matrix2, we need to calculate the root mean square deviation
    rmsd = np.sqrt(np.mean(np.sum((mtx2 - mtx1)**2, axis=1)))
    return rmsd
torch.autograd.set_detect_anomaly(True)

def train(model, data_loader, optimizer, criterion_node, criterion_graph):
    model.train()
    total_loss = 0
    all_distances = []
    batch_count = 0
    hausdorff_distances = []  # To store Hausdorff distances
    procrustes_distances = []  # To store Procrustes distances
    for data in data_loader:
        batch_count += 1
        data = data.to(device)
        optimizer.zero_grad()
        node_predictions, graph_predictions = model(data)
        # print("Node_Predictions are: ",node_predictions.shape)

        node_true_values = data.y.to(device)
        # print("Node_True_Values are: ",node_true_values.shape)
        loss_node = criterion_node(node_predictions, node_true_values)
        graph_targets = data.coaptation.to(device).view(-1, 1)
        loss_graph = criterion_graph(graph_predictions, graph_targets)

        loss = loss_node + loss_graph
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

        batch_distances = np.linalg.norm((node_predictions.cpu().detach().numpy() - node_true_values.cpu().numpy()), axis=1)
        all_distances.extend(batch_distances)

        # Compute Hausdorff distance for each batch and append to the list
        hausdorff_dist = hausdorff_distance(node_predictions.cpu().detach().numpy(), node_true_values.cpu().numpy())
        hausdorff_distances.append(hausdorff_dist)

        # Compute Procrustes distance for each batch and append to the list
        proc_dist = procrustes_distance(node_predictions.cpu().detach().numpy(), node_true_values.cpu().numpy())
        procrustes_distances.append(proc_dist)

    average_distance = np.mean(all_distances) if all_distances else 0
    average_hausdorff = np.mean(hausdorff_distances) if hausdorff_distances else 0
    average_procrustes = np.mean(procrustes_distances) if procrustes_distances else 0
    return total_loss / len(data_loader), average_distance, average_hausdorff, average_procrustes

def evaluate(model, loader, criterion_node, criterion_graph, return_intermediate=False):
    model.eval()
    total_loss = 0
    distances = []
    node_predictions_list = []
    graph_predictions_list = []
    node_true_values_list = []
    graph_true_values_list = []
    intermediate_features_list = []
    hausdorff_distances = []  # To store Hausdorff distances
    procrustes_distances = []  # To store Procrustes distances

    with torch.no_grad():
        for data in loader:
            if return_intermediate:
                intermediate_features = model(data, return_intermediate=True)
                intermediate_features_list.append(intermediate_features.cpu().numpy())
            else:
                node_predictions, graph_predictions = model(data)
            node_predictions, graph_predictions = model(data.to(device))

            node_true_values = data.y.to(device)
            loss_node = criterion_node(node_predictions, node_predictions)
            node_predictions_list.extend(node_predictions.cpu().numpy())
            node_true_values_list.extend(node_true_values.cpu().numpy())
            graph_targets = data.coaptation.to(device).view(-1, 1)
            loss_graph = criterion_graph(graph_predictions, data.coaptation.to(device).view(-1, 1))
            graph_predictions_list.extend(graph_predictions.view(-1).cpu().numpy())
            graph_true_values_list.extend(graph_targets.cpu().numpy())
            total_loss += (loss_node.item() + loss_graph.item()) * data.num_graphs

            # euclidean_distance = np.linalg.norm(node_predictions.cpu().numpy() - node_true_values.cpu().numpy(), axis=1)
            # distances.extend(euclidean_distance)
            # for i in range(len(node_true_values)):
            #     # Euclidean distances
            #     euclidean_distance = np.linalg.norm(node_predictions[i].cpu().numpy() - node_true_values[i].cpu().numpy())
            #     distances.append(euclidean_distance)
            # # Compute and store Hausdorff distance
            # hausdorff_dist = hausdorff_distance(node_predictions.cpu().numpy(), node_true_values.cpu().numpy())
            # hausdorff_distances.append(hausdorff_dist)

            # # Compute Procrustes distance for the entire batch
            # proc_dist = procrustes_distance(node_predictions.cpu().numpy(), node_true_values.cpu().numpy())
            # procrustes_distances.append(proc_dist)
            # Compute Euclidean distances for each point in the batch
            euclidean_distance = np.linalg.norm(node_predictions.cpu().numpy() - node_true_values.cpu().numpy(), axis=1)
            distances.extend(euclidean_distance)

            # Compute Hausdorff distance for the entire batch
            hausdorff_dist = hausdorff_distance(node_predictions.cpu().numpy(), node_true_values.cpu().numpy())
            hausdorff_distances.append(hausdorff_dist)

            # Compute Procrustes distance for the entire batch
            proc_dist = procrustes_distance(node_predictions.cpu().numpy(), node_true_values.cpu().numpy())
            procrustes_distances.append(proc_dist)


    mse_graph = mean_squared_error(graph_true_values_list, graph_predictions_list)
    mae_graph = mean_absolute_error(graph_true_values_list, graph_predictions_list)
    r2_graph = r2_score(graph_true_values_list, graph_predictions_list)
    average_distance = np.mean(distances) if distances else 0
    average_hausdorff = np.mean(hausdorff_distances) if hausdorff_distances else 0
    mean_euclidean = np.mean(distances)
    max_euclidean = np.max(distances)
    min_euclidean = np.min(distances)
    average_procrustes = np.mean(procrustes_distances)
    results= {'total_loss': total_loss / len(loader.dataset), 'MSE Graph': mse_graph, 'MAE Graph':mae_graph, 'R2 Graph':r2_graph, 'average_hausdorff': average_hausdorff,'Graph Predictions List':graph_predictions_list, 'Graph True Values List':graph_true_values_list, 'Node Predictions List':node_predictions_list,'Node True Values':node_true_values_list, 'mean_euclidean': mean_euclidean, 'max_euclidean': max_euclidean, 'min_euclidean': min_euclidean, 'average procustus': average_procrustes}
    if return_intermediate:
        results['code_layer_prediction'] = np.concatenate(intermediate_features_list, axis=0)

    # Ensure to return all necessary values
    return results

def test():
    return evaluate(test_loader)

def write_meshfile(filename, data):
    u_ctrl_pts, v_ctrl_pts = 17, 12  # dimensions for U and V
    with open(filename, 'w') as f:
        f.write('3\n')  # Degree of the NURBS in U direction
        f.write('3 3\n')  # Degree of the NURBS in V direction
        f.write(f'{u_ctrl_pts} {v_ctrl_pts}\n')  # Number of control points in U and V directions
        f.write('0 0 0 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 14 14 14\n')  # U knot vector
        f.write('0 0 0 0 1 2 3 4 5 6 7 8 9 9 9 9\n')  # V knot vector
        for u in range(u_ctrl_pts):
            for v in range(v_ctrl_pts):
                # Access the data using the reshaped indices
                x, y, z = data[u, v]
                f.write(f'{x} {y} {z} 1\n')  # Write with weight 1

def write_data(path, node_predictions, node_true_values, graph_predictions, graph_true_values):
    # folderpath = os.path.join(path, 'GNN_outputs')

    # # Clear the entire folder if it exists
    # if os.path.exists(folderpath):
    #     shutil.rmtree(folderpath)

    # # Create the folder again
    # os.makedirs(folderpath, exist_ok=True)

    for i in range(len(node_predictions)):
        datapoint_path = os.path.join(path, f'datapoint_{i}')
        os.makedirs(datapoint_path, exist_ok=True)

        # Assuming node_predictions[i] is an iterable of parts (e.g., three meshes per datapoint)
        for j in range(len(node_predictions[i])):
            write_meshfile(os.path.join(datapoint_path, f'predicted_deformations_{j}.mesh'), node_predictions[i][j])
            write_meshfile(os.path.join(datapoint_path, f'predicted_deformations_{j}.txt'), node_predictions[i][j])
            write_meshfile(os.path.join(datapoint_path, f'predicted_deformations_{j}.mat'), node_predictions[i][j])
            write_meshfile(os.path.join(datapoint_path, f'true_deformations_{j}.mesh'), node_true_values[i][j])
            write_meshfile(os.path.join(datapoint_path, f'true_deformations_{j}.txt'), node_true_values[i][j])
            write_meshfile(os.path.join(datapoint_path, f'true_deformations_{j}.mat'), node_true_values[i][j])

        # Write graph predictions if they exist
        if graph_predictions is not None and graph_true_values is not None:
            with open(os.path.join(datapoint_path, 'graph_predictions.txt'), 'w') as f:
                f.write(f'Predicted: {graph_predictions[i]}\n')
                f.write(f'True: {graph_true_values[i]}\n')

def writematfile(path, data, predictions, code_layer_pred, data_type):
    datanames = ['inputs_', 'outputs_']
    data_dict = {}
    # Assume data and predictions are organized in tuples or lists of arrays
    for i, datatype in enumerate([data, predictions]):
        for j, array in enumerate(datatype):
            varname = datanames[i] + str(j) + ('_original' if i == 0 else '_predicted')
            data_dict[varname] = array
    data_dict['code_layer_prediction'] = code_layer_pred  # Adding code layer predictions
    io.savemat(os.path.join(path, f'{data_type}_data.mat'), data_dict)  # Save as a .mat file

def reshape_data(data, num_points_per_sample=612):
    # First, determine how many samples are present
    num_samples = data.shape[0] // num_points_per_sample
    # Each sample should be reshaped from (612, 3) to (3, 17, 12, 3)
    reshaped_data = data.reshape(num_samples, 3, 17, 12, 3)
    return reshaped_data

# Now adjust the training and validation calls in your loop
best_val_loss = float('inf')
r2_scores = []  # To store R2 scores for plotting

output_dir = "output_data"
os.makedirs(output_dir, exist_ok=True)
gnn_output_path = os.path.join(output_dir, "GNN_outputs")  # This will be the constant folder for all epochs
os.makedirs(gnn_output_path, exist_ok=True)  # Create the directory once

# checkpoint_path = '/content/output_data/best_model.pth'
# checkpoint = torch.load(checkpoint_path)

# # Load the state into your model
# model.load_state_dict(checkpoint)

for epoch in range(1, 51):
    # Pass all required arguments to the train function
    train_loss, train_dist, train_hausdorff, train_procustus = train(model, train_loader, optimizer, criterion_node, criterion_graph)
    # train_loss, train_dist = train(model, train_loader, optimizer, criterion_node, criterion_graph)
    # Pass all required arguments to the evaluate function, and unpack necessary values
    val_stats = evaluate(model, val_loader, criterion_node, criterion_graph, return_intermediate=True)
    r2_scores.append(val_stats['R2 Graph'])  # Append R2 score for plotting


    print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, Train Avg Euclidean: {train_dist:.4f}, Train Avg Hausdorff: {train_hausdorff:.4f}, Train Procustus Distance: {train_procustus:.4f}')
    print(f"Val Loss: {val_stats['total_loss']:.4f}")
    print(f"Val Euclidean Distance - Mean: {val_stats['mean_euclidean']:.4f}, Hausdorff Distance: {val_stats['average_hausdorff']:.4f}, Average Procustus Distance: {val_stats['average procustus']:.4f}")

    node_predictions = np.array(val_stats['Node Predictions List'])
    node_true_values = np.array(val_stats['Node True Values'])

    # Reshape node predictions and true values immediately after extraction
    # reshaped_node_predictions = reshape_data(node_predictions)
    # reshaped_node_true_values = reshape_data(node_true_values)
    reshaped_node_predictions = reshape_data(np.array(val_stats['Node Predictions List']))
    reshaped_node_true_values = reshape_data(np.array(val_stats['Node True Values']))

    # for i in range(reshaped_node_predictions.shape[0]):
    #         mesh_dir = os.path.join(output_dir, f"epoch_{epoch}_datapoint_{i}")
    #         os.makedirs(mesh_dir, exist_ok=True)
    #         write_meshfile(os.path.join(mesh_dir, 'predicted_deformations.mesh'), reshaped_node_predictions[i])
    #         write_meshfile(os.path.join(mesh_dir, 'true_deformations.mesh'), reshaped_node_true_values[i])

    # for i in range(3):
    #     mesh_dir = os.path.join(output_dir, f"epoch_{epoch}_datapoint_{i}")
    #     os.makedirs(mesh_dir, exist_ok=True)
    #     # write_meshfile(os.path.join(mesh_dir, f'predicted_deformations_{i}.mesh'), reshaped_node_predictions[i])
    #     # write_meshfile(os.path.join(mesh_dir, f'true_deformations_{i}.mesh'), reshaped_node_true_values[i])
    #     write_data(gnn_output_path, reshaped_node_predictions, reshaped_node_true_values,
    #            val_stats.get('Graph Predictions List'), val_stats.get('Graph True Values List'))

        # Save MATLAB data files
    writematfile(output_dir, reshaped_node_true_values, reshaped_node_predictions, val_stats['code_layer_prediction'], f'validation_epoch_{epoch}')

    # Save the best model
    if val_stats['total_loss'] < best_val_loss:
        best_val_loss = val_stats['total_loss']
        best_model_path = os.path.join(output_dir, 'best_model.pth')
        torch.save(model.state_dict(), best_model_path)

best_model_path = '/home/vedant/output_data/best_model.pth'
# Optionally, load and evaluate the best model at the end
model.load_state_dict(torch.load(best_model_path))
test_stats = evaluate(model, test_loader, criterion_node, criterion_graph, return_intermediate=True)
print(f"Test Loss: {test_stats['total_loss']:.4f}")
print(f"Test Euclidean Distance - Mean: {test_stats['mean_euclidean']:.4f}, Test Hausdorff Distance: {test_stats['average_hausdorff']:.4f}, Average Procustus Distance: {test_stats['average procustus']}")
print(f"The R2 Score is: {test_stats['R2 Graph']}")


test_node_predictions = np.array(test_stats['Node Predictions List'])
test_node_true_values = np.array(test_stats['Node True Values'])

reshaped_test_node_predictions = reshape_data(test_node_predictions)
reshaped_test_node_true_values = reshape_data(test_node_true_values)

for i in range(3):
    # test_mesh_dir = os.path.join(output_dir, f"test_datapoint_{i}")
    # os.makedirs(test_mesh_dir, exist_ok=True)
    write_data(gnn_output_path, reshaped_node_predictions, reshaped_node_true_values, test_stats.get('Graph Predictions List'), test_stats.get('Graph True Values List'))

# Optionally, save MATLAB file for test data
writematfile(output_dir, reshaped_test_node_true_values, reshaped_test_node_predictions, test_stats['code_layer_prediction'], 'test')

# Plot R2 scores
plt.figure(figsize=(10, 5))
plt.plot(range(1, 51), r2_scores, label='R2 Score per Epoch', marker='o')
plt.xlabel('Epoch')
plt.ylabel('R2 Score')
plt.title('R2 Score Progression')
plt.legend()
plt.grid(True)
plt.show()

# Scatter plot for True vs Predicted values
plt.figure(figsize=(10, 5))
plt.scatter(val_stats['Graph True Values List'], val_stats['Graph Predictions List'], color='blue', alpha=0.5, label='True vs Predicted')
plt.xlabel('True Values')
plt.ylabel('Predicted Values')
plt.title('True vs Predicted Coaptation Values')
min_val = min(min(test_stats['Graph True Values List']), min(test_stats['Graph Predictions List']))
max_val = max(max(test_stats['Graph True Values List']), max(test_stats['Graph Predictions List']))
plt.plot([min_val, max_val], [min_val, max_val], 'r--', label='Ideal Prediction')
plt.savefig("True vs Predicted Values")
plt.legend()
plt.grid(True)
plt.show()

