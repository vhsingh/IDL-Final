# -*- coding: utf-8 -*-
"""MeshGNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qJBdB1a9ODs0LdX1KlwVLvdowExDeqop
"""

!pip install torch_geometric

import pandas as pd
import torch
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from torch_geometric.data import Data
import os
import pickle
import glob
import shutil
from tqdm import tqdm
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GCNConv, global_mean_pool
from torch_geometric.data import DataLoader
from torch.optim import Adam
import random
from scipy.spatial.distance import directed_hausdorff
from scipy.spatial import procrustes

import os

!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.11.0+cu113.html
!pip install torch-sparse -f https://data.pyg.org/whl/torch-1.11.0+cu113.html
!pip install torch-geometric
!pip install -q git+https://github.com/snap-stanford/deepsnap.git

def clear_folder(folder_path):
    # Check if the folder exists
    if os.path.exists(folder_path):
        # Remove all files and folders within the directory
        for filename in os.listdir(folder_path):
            file_path = os.path.join(folder_path, filename)
            try:
                if os.path.isfile(file_path) or os.path.islink(file_path):
                    os.unlink(file_path)
                elif os.path.isdir(file_path):
                    shutil.rmtree(file_path)
            except Exception as e:
                print('Failed to delete %s. Reason: %s' % (file_path, e))
    else:
        # If the folder doesn't exist, create it
        os.makedirs(folder_path)

def prepare_item_for_excel(item_element):
    # Flatten the single element (item) so each coordinate set (x, y, z) is a row in the output DataFrame
    flattened = item_element.reshape(-1, 3)  # Reshape: Flatten and then organize into rows of 3 values each
    return pd.DataFrame(flattened)

def prepare_1d_item_for_excel(item_element):
    # Prepare the 1D element data as a DataFrame
    return pd.DataFrame(item_element.reshape(-1,1), columns=['Value'])

def process_data(file_path, base_folder, num_1d_data):

    clear_folder(base_folder)

    # Ensure the base folder exists
    if not os.path.exists(base_folder):
        os.makedirs(base_folder)

    # Load the pickle file
    with open(file_path, 'rb') as f:
        data = pickle.load(f)

    # Base path for saving Excel files
    base_excel_file_path = os.path.join(base_folder, 'Element_{}.xlsx')

    # Loop over the first 10 elements
    for element_index in range(18668):
        # Create an Excel writer object for this element
        excel_file_path = base_excel_file_path.format(element_index)
        with pd.ExcelWriter(excel_file_path, engine='openpyxl') as writer:
            # Process each of the first 3 3D data items for this element
            for item_index, item in enumerate(data[:3]):
                item_element = item[element_index]  # This is now a single (17, 12, 3) element
                df = prepare_item_for_excel(item_element)
                df.to_excel(writer, sheet_name=f'Item_{item_index}', index=False)

            # Process the 1D data items for this element based on the file type
            for item_index in range(3, 3 + num_1d_data):
                item_element = data[item_index][element_index]  # This is a single 1D element
                df = prepare_1d_item_for_excel(item_element)
                df.to_excel(writer, sheet_name=f'Property_{item_index-3}', index=False)

# File paths
input_file_path = '/home/vedant/input_data.pkl'
output_file_path = '/home/vedant/output_data.pkl'

# Folder paths
input_folder = '/home/vedant/Input_excel'
output_folder = '/home/vedant/Output_Excel'

# Process each file with respective number of 1D data items
process_data(input_file_path, input_folder, 4)  # Input data has 4 1D data items
process_data(output_file_path, output_folder, 1)  # Output data has 1 1D data item

# from google.colab import drive
# drive.mount('/content/drive')

# Define the paths to the input and output Excel folders
input_folder_path = "/home/vedant/Input_excel"
output_folder_path = "/home/vedant/Output_Excel"

# Get lists of Excel files in both folders
input_files = glob.glob(os.path.join(input_folder_path, '*.xlsx'))
output_files = glob.glob(os.path.join(output_folder_path, '*.xlsx'))

# Sort files to ensure matching order
input_files.sort()
output_files.sort()

# Initialize a dictionary to hold the graph data objects
graph_data_objects = {}

# Define the group size for adjacency pairs
group_size = 17

# Processing input files to create graph data objects
for input_file, output_file in zip(input_files, output_files):
    for sheet_idx in range(3):  # Assuming processing one sheet here for simplicity
        input_df = pd.read_excel(input_file, sheet_name=sheet_idx)
        output_df = pd.read_excel(output_file, sheet_name=sheet_idx)

        # Read additional properties and extract the scalar values
        pressure_value = pd.read_excel(input_file, sheet_name='Property_0')['Value'].iloc[0]
        thickness_value = pd.read_excel(input_file, sheet_name='Property_1')['Value'].iloc[0]
        coaptation_value = pd.read_excel(output_file, sheet_name='Property_0')['Value'].iloc[0]

        # Convert scalar values to tensors
        pressure_tensor = torch.tensor([pressure_value], dtype=torch.float)
        thickness_tensor = torch.tensor([thickness_value], dtype=torch.float)
        coaptation_tensor = torch.tensor([coaptation_value], dtype=torch.float)

        # Extract node coordinates
        x = torch.tensor(input_df.iloc[:, [0, 1, 2]].values, dtype=torch.float)
        y = torch.tensor(output_df.iloc[:, [0, 1, 2]].values, dtype=torch.float)

        src_indices = []
        dst_indices = []
        dx = []  # Differences in x coordinates
        dy = []  # Differences in y coordinates
        dz = []  # Differences in z coordinates

        for i in range(len(input_df)):
            # Add sequential edges
            if (i + 1) % group_size != 0 and (i + 1) < len(input_df):
                src_indices.append(i)
                dst_indices.append(i + 1)
                dx.append(abs(x[i, 0] - x[i+1, 0]).item())
                dy.append(abs(x[i, 1] - x[i+1, 1]).item())
                dz.append(abs(x[i, 2] - x[i+1, 2]).item())

            # Add edges with the group_size interval
            if i < len(input_df) - group_size:
                src_indices.append(i)
                dst_indices.append(i + group_size)
                dx.append(abs(x[i, 0] - x[i+group_size, 0]).item())
                dy.append(abs(x[i, 1] - x[i+group_size, 1]).item())
                dz.append(abs(x[i, 2] - x[i+group_size, 2]).item())

        # Normalize the differences
        max_dx = max(dx) if dx else 1  # Avoid division by zero
        max_dy = max(dy) if dy else 1
        max_dz = max(dz) if dz else 1

        dx_normalized = [d / max_dx for d in dx]
        dy_normalized = [d / max_dy for d in dy]
        dz_normalized = [d / max_dz for d in dz]

        edge_index = torch.tensor([src_indices, dst_indices], dtype=torch.long)
        edge_attr = torch.tensor(list(zip(dx_normalized, dy_normalized, dz_normalized)), dtype=torch.float)  # Combine normalized differences into a 2D tensor

        # Create graph data object with normalized edge attributes
        graph_data_object = Data(x=x, edge_index=edge_index,
                                 edge_attr=edge_attr,y=y,
                                 pressure=pressure_tensor,
                                 thickness=thickness_tensor,
                                 coaptation=coaptation_tensor)

        item_name = f"{os.path.basename(input_file)}_Sheet_{sheet_idx+1}"
        graph_data_objects[item_name] = graph_data_object

print("Number of graph data objects created:", len(graph_data_objects))

data_list = list(graph_data_objects.values())

import torch
import random
import pandas as pd
import torch_scatter
import torch.nn as nn
from torch.nn import Linear, Sequential, LayerNorm, ReLU
from torch_geometric.nn.conv import MessagePassing
from torch_geometric.data import DataLoader

import numpy as np
import time
import torch.optim as optim
from tqdm import trange
import pandas as pd
import copy
import matplotlib.pyplot as plt
from torch_scatter import scatter_mean

def normalize(to_normalize,mean_vec,std_vec):
    return (to_normalize-mean_vec)/std_vec

def unnormalize(to_unnormalize,mean_vec,std_vec):
    return to_unnormalize*std_vec+mean_vec

def get_stats(data_list):
    '''
    Method for normalizing processed datasets. Given  the processed data_list,
    calculates the mean and standard deviation for the node features, edge features,
    and node outputs, and normalizes these using the calculated statistics.
    '''

    #mean and std of the node features are calculated
    mean_vec_x=torch.zeros(data_list[0].x.shape[1:])
    std_vec_x=torch.zeros(data_list[0].x.shape[1:])

    #mean and std of the edge features are calculated
    mean_vec_edge=torch.zeros(data_list[0].edge_attr.shape[1:])
    std_vec_edge=torch.zeros(data_list[0].edge_attr.shape[1:])

    #mean and std of the output parameters are calculated
    mean_vec_y=torch.zeros(data_list[0].y.shape[1:])
    std_vec_y=torch.zeros(data_list[0].y.shape[1:])

    #Define the maximum number of accumulations to perform such that we do
    #not encounter memory issues
    max_accumulations = 10**6

    #Define a very small value for normalizing to
    eps=torch.tensor(1e-8)

    #Define counters used in normalization
    num_accs_x = 0
    num_accs_edge=0
    num_accs_y=0

    #Iterate through the data in the list to accumulate statistics
    for dp in data_list:

        #Add to the
        mean_vec_x+=torch.sum(dp.x,dim=0)
        std_vec_x+=torch.sum(dp.x**2,dim=0)
        num_accs_x+=dp.x.shape[0]

        mean_vec_edge+=torch.sum(dp.edge_attr,dim=0)
        std_vec_edge+=torch.sum(dp.edge_attr**2,dim=0)
        num_accs_edge+=dp.edge_attr.shape[0]

        mean_vec_y+=torch.sum(dp.y,dim=0)
        std_vec_y+=torch.sum(dp.y**2,dim=0)
        num_accs_y+=dp.y.shape[0]

        if(num_accs_x>max_accumulations or num_accs_edge>max_accumulations or num_accs_y>max_accumulations):
            break

    mean_vec_x = mean_vec_x/num_accs_x
    std_vec_x = torch.maximum(torch.sqrt(std_vec_x/num_accs_x - mean_vec_x**2),eps)

    mean_vec_edge = mean_vec_edge/num_accs_edge
    std_vec_edge = torch.maximum(torch.sqrt(std_vec_edge/num_accs_edge - mean_vec_edge**2),eps)

    mean_vec_y = mean_vec_y/num_accs_y
    std_vec_y = torch.maximum(torch.sqrt(std_vec_y/num_accs_y - mean_vec_y**2),eps)

    mean_std_list=[mean_vec_x,std_vec_x,mean_vec_edge,std_vec_edge,mean_vec_y,std_vec_y]

    return mean_std_list

class GraphDecoder(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(GraphDecoder, self).__init__()
        # +2 for the pressure and thickness values
        self.fc1 = nn.Linear(input_dim + 2, 64)
        self.fc2 = nn.Linear(64, output_dim)
        self.relu = nn.ReLU()

    def forward(self, graph_features, pressure, thickness):
        # Pressure and thickness must be reshaped if they are not in the correct dimension
        pressure = pressure.unsqueeze(1) if pressure.dim() == 1 else pressure
        thickness = thickness.unsqueeze(1) if thickness.dim() == 1 else thickness
        # Concatenate graph features, pressure, and thickness
        combined_features = torch.cat([graph_features, pressure, thickness], dim=1)
        x = self.fc1(combined_features)
        x = self.relu(x)
        x = self.fc2(x)
        return x

class MeshGraphNet(torch.nn.Module):
    def __init__(self, input_dim_node, input_dim_edge, hidden_dim, output_dim, num_layers):
        super(MeshGraphNet, self).__init__()
        self.num_layers = args.num_layers

        # Node and edge encoders
        self.node_encoder = Sequential(
            Linear(input_dim_node, hidden_dim),
            ReLU(),
            Linear(hidden_dim, hidden_dim),
            LayerNorm(hidden_dim)
        )

        self.edge_encoder = Sequential(
            Linear(input_dim_edge, hidden_dim),
            ReLU(),
            Linear(hidden_dim, hidden_dim),
            LayerNorm(hidden_dim)
        )

        # Message passing layers
        self.processor = nn.ModuleList()
        assert (self.num_layers >= 1), 'Number of message passing layers is not >=1'

        processor_layer=self.build_processor_model()
        for _ in range(self.num_layers):
            self.processor.append(processor_layer(hidden_dim,hidden_dim))

        # Decoders
        self.node_decoder = Sequential(
            Linear(hidden_dim, output_dim),
            ReLU()
        )

        self.graph_decoder = GraphDecoder(hidden_dim, 1)

    def build_processor_model(self):
        return ProcessorLayer

    def forward(self, data, mean_vec_x, std_vec_x, mean_vec_edge, std_vec_edge):
      x, edge_index, edge_attr, batch, coaptation_values = data.x, data.edge_index, data.edge_attr, data.batch, data.coaptation
      pressure = data.pressure
      thickness = data.thickness

      # Normalize and encode features into embeddings
      x = normalize(x, mean_vec_x, std_vec_x)
      edge_attr = normalize(edge_attr, mean_vec_edge, std_vec_edge)
      x = self.node_encoder(x)
      edge_attr = self.edge_encoder(edge_attr)

      # Message passing
      for i in range(self.num_layers):
          x,edge_attr = self.processor[i](x,edge_index,edge_attr)
          assert isinstance(x, torch.Tensor), f"After processing, x became a {type(x)}"

      # Node-level predictions
      node_predictions = self.node_decoder(x)

      # Graph-level predictions, using mean pooling
      graph_features = scatter_mean(x, batch, dim=0)  # Aggregate node features to graph features
      graph_predictions = self.graph_decoder(graph_features, pressure.to(x.device), thickness.to(x.device))

      return node_predictions, graph_predictions

class ProcessorLayer(MessagePassing):
    def __init__(self, in_channels, out_channels,  **kwargs):
        super(ProcessorLayer, self).__init__(  **kwargs )
        """
        in_channels: dim of node embeddings [128], out_channels: dim of edge embeddings [128]

        """

        # Note that the node and edge encoders both have the same hidden dimension
        # size. This means that the input of the edge processor will always be
        # three times the specified hidden dimension
        # (input: adjacent node embeddings and self embeddings)
        self.edge_mlp = Sequential(Linear( 3* in_channels , out_channels),
                                   ReLU(),
                                   Linear( out_channels, out_channels),
                                   LayerNorm(out_channels))

        self.node_mlp = Sequential(Linear( 2* in_channels , out_channels),
                                   ReLU(),
                                   Linear( out_channels, out_channels),
                                   LayerNorm(out_channels))
        self.reset_parameters()

    def reset_parameters(self):
        """
        reset parameters for stacked MLP layers
        """
        self.edge_mlp[0].reset_parameters()
        self.edge_mlp[2].reset_parameters()
        # self.edge_mlp[1].reset_parameters()

        self.node_mlp[0].reset_parameters()
        self.node_mlp[2].reset_parameters()
        # self.node_mlp[1].reset_parameters()

    def forward(self, x, edge_index, edge_attr, size=None):
      # Ensure that inputs are tensors
      assert isinstance(x, torch.Tensor), f"Expected x to be a Tensor, got {type(x)}"
      assert isinstance(edge_attr, torch.Tensor), f"Expected edge_attr to be a Tensor, got {type(edge_attr)}"

      out, updated_edges = self.propagate(edge_index, x=x, edge_attr=edge_attr, size=size)

      # Ensure outputs are tensors
      assert isinstance(out, torch.Tensor), f"Expected out to be a Tensor, got {type(out)}"
      assert isinstance(updated_edges, torch.Tensor), f"Expected updated_edges to be a Tensor, got {type(updated_edges)}"

      updated_nodes = torch.cat([x, out], dim=1)
      updated_nodes = x + self.node_mlp(updated_nodes)

      return updated_nodes, updated_edges


    def message(self, x_i, x_j, edge_attr):
        """
        source_node: x_i has the shape of [E, in_channels]
        target_node: x_j has the shape of [E, in_channels]
        target_edge: edge_attr has the shape of [E, out_channels]

        The messages that are passed are the raw embeddings. These are not processed.
        """

        # print("Updated edges before concatenation is:", updated_edges.shape)
        updated_edges=torch.cat([x_i, x_j, edge_attr], dim = 1) # tmp_emb has the shape of [E, 3 * in_channels]
        # print(f"Shape inside message before MLP: {updated_edges.shape}")
        updated_edges=self.edge_mlp(updated_edges)+edge_attr
        # print(f"Shape of updated_edges inside message: {updated_edges.shape}")

        return updated_edges

    def aggregate(self, updated_edges, edge_index, dim_size = None):
        """
        First we aggregate from neighbors (i.e., adjacent nodes) through concatenation,
        then we aggregate self message (from the edge itself). This is streamlined
        into one operation here.
        """

        # The axis along which to index number of nodes.
        node_dim = 0

        # out = torch_scatter.scatter(updated_edges, edge_index[0, :], dim=node_dim, reduce = 'sum')
        # print(f"Shape of inputs in aggregate: {updated_edges.shape}, Output shape: {out.shape}")

        out = torch_scatter.scatter(updated_edges, edge_index[0], dim=node_dim, reduce='sum', dim_size=dim_size)

        # print(f"Shape of inputs in aggregate: {updated_edges.shape}, Output shape: {out.shape}")

        return out, updated_edges

def build_optimizer(args, params):
    weight_decay = args.weight_decay
    filter_fn = filter(lambda p : p.requires_grad, params)
    if args.opt == 'adam':
        optimizer = optim.Adam(filter_fn, lr=args.lr, weight_decay=weight_decay)
    elif args.opt == 'sgd':
        optimizer = optim.SGD(filter_fn, lr=args.lr, momentum=0.95, weight_decay=weight_decay)
    elif args.opt == 'rmsprop':
        optimizer = optim.RMSprop(filter_fn, lr=args.lr, weight_decay=weight_decay)
    elif args.opt == 'adagrad':
        optimizer = optim.Adagrad(filter_fn, lr=args.lr, weight_decay=weight_decay)
    if args.opt_scheduler == 'none':
        return None, optimizer
    elif args.opt_scheduler == 'step':
        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.opt_decay_step, gamma=args.opt_decay_rate)
    elif args.opt_scheduler == 'cos':
        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.opt_restart)
    return scheduler, optimizer

criterion_node = nn.MSELoss()
criterion_graph = nn.MSELoss()

dataset = list(graph_data_objects.values())
total_length = len(data_list)
print(total_length)
train_size = int(0.9 * total_length)
print(train_size)
val_size = int(0.05 * total_length)
print(val_size)

# Split the data
train_data = data_list[:train_size]
print(len(train_data))
val_data = data_list[train_size:train_size + val_size]
print(len(val_data))
test_data = data_list[train_size + val_size:]

from scipy.spatial.distance import directed_hausdorff
from scipy.spatial import procrustes

def hausdorff_distance(u, v):
    return max(directed_hausdorff(u, v)[0], directed_hausdorff(v, u)[0])

def procrustes_distance(matrix1, matrix2):
    mtx1, mtx2, disparity = procrustes(matrix1, matrix2)
    rmsd = np.sqrt(np.mean(np.sum((mtx2 - mtx1) ** 2, axis=1)))
    return rmsd

from sklearn.metrics import r2_score
def train(dataset, device, stats_list, args):
    '''
    Performs a training loop on the dataset for MeshGraphNets. Also calls
    test and validation functions.
    '''

    df = pd.DataFrame(columns=['epoch','train_loss','test_loss', 'velo_val_loss'])

    #Define the model name for saving
    model_name='model_nl'+str(args.num_layers)+'_bs'+str(args.batch_size) + \
               '_hd'+str(args.hidden_dim)+'_ep'+str(args.epochs)+'_wd'+str(args.weight_decay) + \
               '_lr'+str(args.lr)+'_shuff_'+str(args.shuffle)+'_tr'+str(args.train_size)+'_te'+str(args.test_size)


    #torch_geometric DataLoaders are used for handling the data of lists of graphs
    loader = DataLoader(train_data, batch_size=args.batch_size, shuffle=False)
    test_loader = DataLoader(test_data, batch_size=args.batch_size, shuffle=False)
    print("Test loader defined with", len(test_loader), "batches.")

    #The statistics of the data are decomposed
    [mean_vec_x,std_vec_x,mean_vec_edge,std_vec_edge,mean_vec_y,std_vec_y] = stats_list
    (mean_vec_x,std_vec_x,mean_vec_edge,std_vec_edge,mean_vec_y,std_vec_y)=(mean_vec_x.to(device),
        std_vec_x.to(device),mean_vec_edge.to(device),std_vec_edge.to(device),mean_vec_y.to(device),std_vec_y.to(device))

    # build model
    num_node_features = dataset[0].x.shape[1]
    num_edge_features = dataset[0].edge_attr.shape[1]
    num_classes = 3 # the dynamic variables have the shape of 2 (velocity)

    model = MeshGraphNet(num_node_features, num_edge_features, args.hidden_dim, num_classes,
                            args).to(device)
    scheduler, opt = build_optimizer(args, model.parameters())

    # train
    losses = []
    test_losses = []
    velo_val_losses = []
    best_test_loss = float('inf')
    best_model = None
    node_losses = []
    graph_losses = []
    average_losses = []
    hausdorff_distances, procrustes_distances, all_distances = [], [], []
    for epoch in trange(args.epochs, desc="Training", unit="Epochs"):
      total_node_loss = 0
      total_loss = 0
      total_graph_loss = 0
      model.train()
      for data in loader:
          data = data.to(device)
          opt.zero_grad()
          node_pred, graph_pred = model(data, mean_vec_x, std_vec_x, mean_vec_edge, std_vec_edge)
          # node_loss, graph_loss = model.loss((node_pred, graph_pred), batch, mean_vec_y, std_vec_y, coaptation_value)
          # total_loss = node_loss + graph_loss  # Combine node and graph losses
          # total_loss.backward()
          node_true_values = data.y.to(device)
          loss_node = criterion_node(node_pred, node_true_values)
          graph_targets = data.coaptation.to(device).view(-1, 1)
          loss_graph = criterion_graph(graph_pred, graph_targets)
          loss = loss_node + loss_graph
          loss.backward()
          # Print gradients
          # for name, param in model.named_parameters():
          #     if param.grad is not None:
          #         print(f"{name} gradient mean: {param.grad.mean()}")
          #     else:
          #         print(f"{name} has no gradients")
          opt.step()
          # total_node_loss += node_loss.item()
          # total_graph_loss += graph_loss.item()
          total_loss += loss.item()
          total_node_loss += loss_node.item()
          total_graph_loss += loss_graph.item()
          # Calculate and print distances
          batch_distances = np.linalg.norm((node_predictions.cpu().detach().numpy() - node_true_values.cpu().numpy()), axis=1)
          all_distances.extend(batch_distances)
          print(f"Batch Euclidean distances: {batch_distances}")

          hausdorff_dist = hausdorff_distance(node_predictions.cpu().detach().numpy(), node_true_values.cpu().numpy())
          hausdorff_distances.append(hausdorff_dist)
          print(f"Batch Hausdorff distance: {hausdorff_dist}")

          proc_dist = procrustes_distance(node_predictions.cpu().detach().numpy(), node_true_values.cpu().numpy())
          procrustes_distances.append(proc_dist)
          print(f"Batch Procrustes distance: {proc_dist}")

      avg_distance = np.mean(all_distances)
      avg_hausdorff = np.mean(hausdorff_distances)
      avg_procrustes = np.mean(procrustes_distances)
      print(f"Average Euclidean distance over epoch: {avg_distance}")
      print(f"Average Hausdorff distance over epoch: {avg_hausdorff}")
      print(f"Average Procrustes distance over epoch: {avg_procrustes}")

      # print(f"Epoch {epoch}: Avg Loss (scalar): {avg_loss}")
      # print(type(avg_graph_loss))

      if epoch % 10 == 0:
        test_loss, node_predictions, graph_predictions, graph_labels = test(
        test_loader, device, model, mean_vec_x, std_vec_x, mean_vec_edge,
        std_vec_edge, mean_vec_y, std_vec_y)
        test_losses.append(test_loss)

        if not os.path.isdir(args.checkpoint_dir):
            os.makedirs(args.checkpoint_dir, exist_ok=True)
        if test_loss < best_test_loss:
            best_test_loss = test_loss
            best_model = copy.deepcopy(model)
            torch.save(best_model.state_dict(), os.path.join(args.checkpoint_dir, model_name + '.pt'))

    return test_losses, average_losses, best_model, best_test_loss, test_loader, hausdorff_distances, procrustes_distances

def test(loader, device, test_model, mean_vec_x, std_vec_x, mean_vec_edge, std_vec_edge, mean_vec_y, std_vec_y):
    total_loss = 0
    num_batches = 0
    all_node_predictions = []
    all_graph_predictions = []
    all_graph_labels = []
    all_euclidean_distances = []

    test_model.eval()
    for data in loader:
        data = data.to(device)
        with torch.no_grad():
            node_predictions, graph_predictions = test_model(data, mean_vec_x, std_vec_x, mean_vec_edge, std_vec_edge)
            node_labels = normalize(data.y, mean_vec_y, std_vec_y)
            # Make sure the graph_labels tensor is the same length as the number of graphs in graph_predictions
            graph_labels = data.coaptation.to(device).view(-1,1)

            loss_node = criterion_node(node_predictions, node_labels)
            loss_graph = criterion_graph(graph_predictions, graph_labels)
            total_loss += loss_node.item() + loss_graph.item()
            num_batches += 1

            all_node_predictions.append(node_predictions.cpu().numpy())
            all_graph_predictions.append(graph_predictions.cpu().numpy())
            all_graph_labels.append(graph_labels.cpu().numpy())

            # Compute and print Euclidean distances for each batch
            batch_euclidean_distances = np.linalg.norm((node_predictions.cpu().numpy() - node_labels.cpu().numpy()), axis=1)
            all_euclidean_distances.extend(batch_euclidean_distances)
            print(f"Test Batch Euclidean distances: {batch_euclidean_distances}")

            # Compute Hausdorff and Procrustes distances
            hausdorff_dist = hausdorff_distance(node_predictions.cpu().numpy(), node_labels.cpu().numpy())
            proc_dist = procrustes_distance(node_predictions.cpu().numpy(), node_labels.cpu().numpy())
            print(f"Test Batch Hausdorff distance: {hausdorff_dist}")
            print(f"Test Batch Procrustes distance: {proc_dist}")

            # print(f"Graph Predictions Shape: {graph_predictions.shape}")
            # print(f"Graph Labels Shape: {graph_labels.shape}")

    average_loss = total_loss / num_batches
    all_graph_predictions = np.vstack(all_graph_predictions)
    all_graph_labels = np.vstack(all_graph_labels)  # Ensure all graph labels are correctly formatted
    avg_euclidean = np.mean(all_euclidean_distances) if all_euclidean_distances else 0
    print(f"Average test loss: {average_loss}")
    print(f"Average test Euclidean distance: {avg_euclidean}")
    return average_loss, all_node_predictions, all_graph_predictions, all_graph_labels

class objectview(object):
    def __init__(self, d):
        self.__dict__ = d

for args in [
        {'model_type': 'meshgraphnet',
         'num_layers': 10,
         'batch_size': 16,
         'hidden_dim': 10,
         'epochs': 500,
         'opt': 'adam',
         'opt_scheduler': 'none',
         'opt_restart': 0,
         'weight_decay': 5e-4,
         'lr': 0.001,
         'train_size': 45,
         'test_size': 10,
         'device':'cuda',
         'shuffle': True,
         'save_velo_val': True,
         'save_best_model': True,
         'checkpoint_dir': './best_models/',
         'postprocess_dir': './2d_loss_plots/'},
    ]:
        args = objectview(args)

torch.manual_seed(5)  #Torch
random.seed(5)        #Python
np.random.seed(5)     #NumPy


if(args.shuffle):
  random.shuffle(dataset)

stats_list = get_stats(dataset)

device = 'cuda' if torch.cuda.is_available() else 'cpu'
args.device = device
print(device)

test_losses, average_losses, best_model, best_test_loss, test_loader, all_hausdroff, all_procustus = train(train_data, device, stats_list, args)

print("Min test set loss: {0}".format(min(test_losses)))
# print("Min Node loss: {0}".format(min(node_losses)))
# print("All Graph Losses:", graph_losses)
# print("Graph Losses List Content:", graph_losses)
# print("Type of first element (if exists):", type(graph_losses[0]) if graph_losses else "Empty list")
# print("Minimum Graph loss:", min(graph_losses))
# print("Minimum Graph loss: {0}".format(min(graph_losses)))  # This should now work without error
# if (args.save_velo_val):
#     print("Minimum velocity validation loss: {0}".format(min(velo_val_losses)))
num_node_features = dataset[0].x.shape[1]
num_edge_features = dataset[0].edge_attr.shape[1]
num_classes = 3 # the dynamic variables have the shape of 2 (velocity)
model = MeshGraphNet(num_node_features, num_edge_features, args.hidden_dim, num_classes,
                            args).to(device)

[mean_vec_x,std_vec_x,mean_vec_edge,std_vec_edge,mean_vec_y,std_vec_y] = stats_list
(mean_vec_x,std_vec_x,mean_vec_edge,std_vec_edge,mean_vec_y,std_vec_y)=(mean_vec_x.to(device),std_vec_x.to(device),mean_vec_edge.to(device),std_vec_edge.to(device),mean_vec_y.to(device),std_vec_y.to(device))
# Final evaluation after training
test_loss, node_predictions, graph_predictions, graph_labels = test(test_loader, device, best_model, mean_vec_x, std_vec_x, mean_vec_edge, std_vec_edge, mean_vec_y, std_vec_y)

# Flatten the graph predictions and labels for R2 score calculation
graph_predictions_flat = graph_predictions.flatten()  # Convert it to a 1D array

graph_labels_flat = graph_labels.flatten()  # Convert it to a 1D array
# print(graph_predictions)
# print(graph_labels)

# print(type(graph_labels_flat))  # Check type to ensure it's a numpy.ndarray
# print(type(graph_predictions_flat))  # Check type to ensure it's a numpy.ndarray

# Calculate and print R2 score for the graph predictions
r2 = r2_score(graph_labels_flat, graph_predictions_flat)
print(f"R2 Score for Graph Predictions: {r2}")

